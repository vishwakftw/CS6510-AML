{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24712 19\n"
     ]
    }
   ],
   "source": [
    "# Data Processing\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, normalize, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "train_data = np.genfromtxt('trainData.csv', delimiter=',', dtype=str)\n",
    "m = train_data.shape[0] - 1\n",
    "d = train_data.shape[1] - 1\n",
    "print(m, d)\n",
    "train_input_raw, train_output = train_data[1:,0:d].tolist(), train_data[1:,d].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above step uses `numpy`'s `genfromtxt` function since there are no missing values. Below I randomly print to get the actual types of data like textual information and specific fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['28', '\"admin.\"', '\"single\"', '\"university.degree\"', '\"no\"', '\"yes\"', '\"no\"', '\"cellular\"', '\"aug\"', '\"mon\"', '1', '999', '0', '\"nonexistent\"', '-2.9', '92.201', '-31.4', '0.861', '5076.2'], '1']\n"
     ]
    }
   ],
   "source": [
    "print([train_input_raw[0], train_output[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing step 1: \n",
    "Remove quotes in textual information\n",
    "\n",
    "#### Preprocessing step 2:\n",
    "Convert numeric information to numbers as opposed to within single quotes. These fields are `0, 10, 11, 12, 14, ..., 19`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28.0, 'admin.', 'single', 'university.degree', 'no', 'yes', 'no', 'cellular', 'aug', 'mon', 1.0, 999.0, 0.0, 'nonexistent', -2.9, 92.201, -31.4, 0.861, '5076.2']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, m):\n",
    "    for j in range(0, d-1):\n",
    "        if train_input_raw[i][j].find(\"\\\"\") != -1:\n",
    "            train_input_raw[i][j] = train_input_raw[i][j][1:-1] # removing double quotes in pairs\n",
    "        else:\n",
    "            train_input_raw[i][j] = float(train_input_raw[i][j])\n",
    "    train_output[i] = float(train_output[i])\n",
    "train_output = np.array(train_output).astype(float)\n",
    "\n",
    "print(train_input_raw[0])\n",
    "print(train_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the preprocessing has been completed, it is time to separate the input and outputs and begin training. Clearly, we might have to go with Ensemble methods since the data has both textual and numeric information in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28.0, 'student', 'single', 'university.degree', 'no', 'yes', 'no', 'telephone', 'aug', 'wed', 1.0, 999.0, 0.0, 'nonexistent', -1.7, 94.027, -38.3, 0.89, '4991.6']\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "rand_index = np.random.randint(low=0, high=m)\n",
    "print(train_input_raw[rand_index])\n",
    "print(train_output[rand_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the data through a simple level of preprocessing, I am going to convert text data into numeric values. For this I am first going to tokenize and then merge this into the actual training input.\n",
    "\n",
    "#### Preprocessing step 3:\n",
    "Convert the text data into numbers using `LabelEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training text data : (24712, 10)\n",
      "Size of training text data: (24712, 10)\n",
      "Text: ['admin.' 'single' 'university.degree' 'no' 'yes' 'no' 'cellular' 'aug'\n",
      " 'mon' 'nonexistent'] --> Data: [0 2 6 0 2 0 0 1 1 1]\n",
      "Text: ['blue-collar' 'single' 'basic.9y' 'no' 'yes' 'no' 'cellular' 'jul' 'tue'\n",
      " 'nonexistent'] --> Data: [1 2 2 0 2 0 0 3 3 1]\n"
     ]
    }
   ],
   "source": [
    "train_text_data = np.hstack((np.array(train_input_raw)[:,1:10], np.array(train_input_raw)[:,13:14]))\n",
    "print('Size of training text data : {0}'.format(train_text_data.shape))\n",
    "\n",
    "# Converting the textual data into one vector per training input\n",
    "train_text_vectors = []\n",
    "\n",
    "for j in range(0, train_text_data.shape[1]):\n",
    "    lbl_enc = LabelEncoder()\n",
    "    train_text_vectors.append(lbl_enc.fit_transform(train_text_data[:,j]))\n",
    "train_text_vectors = np.array(train_text_vectors).T\n",
    "\n",
    "print('Size of training text data: {0}'.format(train_text_vectors.shape))\n",
    "print('Text: {0} --> Data: {1}'.format(train_text_data[0], train_text_vectors[0]))\n",
    "print('Text: {0} --> Data: {1}'.format(train_text_data[10], train_text_vectors[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to make five versions of the dataset\n",
    "+ Unnormalized\n",
    "+ Normalized\n",
    "+ Feature Scaled\n",
    "+ Feature Scaled + Normalized\n",
    "+ One Hot Encoded text + Normalized\n",
    "\n",
    "I use the `OneHotEncoder` to encode categorical data after encoding using label as shown [here](http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 1: Shape (24712, 19)\n",
      "Vector = [  2.80000000e+01   8.00000000e+00   2.00000000e+00   6.00000000e+00\n",
      "   0.00000000e+00   2.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "   1.00000000e+00   4.00000000e+00   1.00000000e+00   9.99000000e+02\n",
      "   0.00000000e+00   1.00000000e+00  -1.70000000e+00   9.40270000e+01\n",
      "  -3.83000000e+01   8.90000000e-01   4.99160000e+03]: \n",
      "Norm = 5091.688836607064\n",
      "Version 2: Shape (24712, 19)\n",
      "Vector = [  5.49915773e-03   1.57118792e-03   3.92796980e-04   1.17839094e-03\n",
      "   0.00000000e+00   3.92796980e-04   0.00000000e+00   1.96398490e-04\n",
      "   1.96398490e-04   7.85593961e-04   1.96398490e-04   1.96202092e-01\n",
      "   0.00000000e+00   1.96398490e-04  -3.33877433e-04   1.84667608e-02\n",
      "  -7.52206217e-03   1.74794656e-04   9.80342704e-01]: \n",
      "Norm = 1.0\n",
      "Version 3: Shape (24712, 19)\n",
      "Vector = [-1.1518788   1.18508251  1.36077868  1.05604046 -0.51183526  0.94129362\n",
      " -0.44957775  1.32024041 -1.38492557  1.44061428 -0.56425354  0.19653049\n",
      " -0.35249416  0.19488942 -1.13794396  0.7799893   0.48107921 -1.57424294\n",
      " -2.42950204]: \n",
      "Norm = 4.880515947065891\n",
      "(24712, 19)\n",
      "Version 4: Shape (24712, 19)\n",
      "Vector = [-0.23601579  0.2428191   0.27881861  0.21637886 -0.10487319  0.19286765\n",
      " -0.09211685  0.27051247 -0.28376622  0.29517664 -0.1156135   0.04026838\n",
      " -0.07222477  0.03993213 -0.23316059  0.15981698  0.09857138 -0.32255666\n",
      " -0.49779615]: \n",
      "Norm = 1.0\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  1.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.]\n",
      "Version 5: Shape (24712, 62)\n",
      "Vector = [  5.49917013e-03   1.96398933e-04   1.96202534e-01   0.00000000e+00\n",
      "  -3.33878187e-04   1.84668025e-02  -7.52207915e-03   1.74795051e-04\n",
      "   9.80344916e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.96398933e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.96398933e-04\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.96398933e-04\n",
      "   0.00000000e+00   1.96398933e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.96398933e-04   1.96398933e-04\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.96398933e-04\n",
      "   0.00000000e+00   1.96398933e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.96398933e-04   0.00000000e+00\n",
      "   1.96398933e-04   0.00000000e+00]: \n",
      "Norm = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Version 1\n",
    "train_input_1 = np.hstack((np.array(train_input_raw)[:,0:1], \n",
    "                         train_text_vectors[:,0:train_text_vectors.shape[1] - 1], \n",
    "                         np.array(train_input_raw)[:,10:13],\n",
    "                         train_text_vectors[:,train_text_vectors.shape[1] - 1].reshape(-1, 1),\n",
    "                         np.array(train_input_raw)[:,14:]\n",
    "                        )).astype(float)\n",
    "print(\"Version 1: Shape {0}\".format(train_input_1.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}'.format(train_input_1[rand_index], np.linalg.norm(train_input_1[rand_index])))\n",
    "\n",
    "# Version 2\n",
    "train_input_2 = normalize(train_input_1)\n",
    "print(\"Version 2: Shape {0}\".format(train_input_2.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}'.format(train_input_2[rand_index], np.linalg.norm(train_input_2[rand_index])))\n",
    "\n",
    "# Version 3\n",
    "mean_reduce = StandardScaler()\n",
    "train_input_3 = mean_reduce.fit_transform(train_input_1)\n",
    "print(\"Version 3: Shape {0}\".format(train_input_3.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}'.format(train_input_3[rand_index], np.linalg.norm(train_input_3[rand_index])))\n",
    "\n",
    "# Version 4\n",
    "train_input_4 = normalize(train_input_3)\n",
    "print(train_input_4.shape)\n",
    "print(\"Version 4: Shape {0}\".format(train_input_4.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}'.format(train_input_4[rand_index], np.linalg.norm(train_input_4[rand_index])))\n",
    "\n",
    "# Version 5\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(train_text_vectors)\n",
    "train_text_ohe_vectors = ohe.transform(train_text_vectors).toarray()\n",
    "print(train_text_ohe_vectors[0])\n",
    "\n",
    "train_input_5 = np.hstack((np.array(train_input_raw)[:,0:1],\n",
    "                         np.array(train_input_raw)[:,10:13],\n",
    "                         np.array(train_input_raw)[:,14:],\n",
    "                         train_text_ohe_vectors\n",
    "                         )).astype(float)\n",
    "train_input_5 = normalize(train_input_5)\n",
    "print(\"Version 5: Shape {0}\".format(train_input_5.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}'.format(train_input_5[rand_index], np.linalg.norm(train_input_5[rand_index])))\n",
    "\n",
    "train_input = (train_input_1, train_input_2, train_input_3, train_input_4, train_input_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the preprocessing steps are completed. I will try my preprocessed dataset on multiple methods of classification.\n",
    "For this, I will generate 5 folds (stratified) and use 5-fold cross validation to get the best method. Folds should be invariant, hence I will use `train_input_1` and its output to generate the `folds` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\tTrain number: 19769 Validation number: 4943\n",
      "Fold 2\tTrain number: 19769 Validation number: 4943\n",
      "Fold 3\tTrain number: 19769 Validation number: 4943\n",
      "Fold 4\tTrain number: 19770 Validation number: 4942\n",
      "Fold 5\tTrain number: 19771 Validation number: 4941\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "folds = []\n",
    "for tr_split, va_split in skf.split(train_input_1, train_output):\n",
    "    folds.append((tr_split, va_split))\n",
    "for i in range(0, len(folds)):\n",
    "    print(\"Fold {0}\\tTrain number: {1} Validation number: {2}\".format(i+1, len(folds[i][0]), len(folds[i][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1: SVM with Gaussian Kernel with 5 fold cross validation on all versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for v in range(0, 5):\n",
    "    accu_train = prec_train = rcll_train = auc_train = 0.0\n",
    "    accu_valid = prec_valid = rcll_valid = auc_valid = 0.0\n",
    "    g_svm = SVC(kernel='rbf', cache_size=2000)\n",
    "    for i in range(0, len(folds)):\n",
    "        print(\"Performing fold {0}\".format(i+1))\n",
    "        g_svm.fit(train_input[v][folds[i][0]], train_output[folds[i][0]])\n",
    "\n",
    "        train_pred = g_svm.predict(train_input[v][folds[i][0]])\n",
    "        valid_pred = g_svm.predict(train_input[v][folds[i][1]])\n",
    "\n",
    "        accu_train += accuracy_score(train_output[folds[i][0]], train_pred)\n",
    "        prec_train += precision_score(train_output[folds[i][0]], train_pred)\n",
    "        rcll_train += recall_score(train_output[folds[i][0]], train_pred)\n",
    "        auc_train += roc_auc_score(train_output[folds[i][0]], train_pred)\n",
    "    \n",
    "        accu_valid += accuracy_score(train_output[folds[i][1]], valid_pred)\n",
    "        prec_valid += precision_score(train_output[folds[i][1]], valid_pred)\n",
    "        rcll_valid += recall_score(train_output[folds[i][1]], valid_pred)\n",
    "        auc_valid += roc_auc_score(train_output[folds[i][1]], valid_pred)\n",
    "    \n",
    "    print(\"Average training accuracy after {0} fold cross validation = {1}\".format(len(folds), accu_train/len(folds)))\n",
    "    print(\"Average training precision after {0} fold cross validation = {1}\".format(len(folds), prec_train/len(folds)))\n",
    "    print(\"Average training recall after {0} fold cross validation = {1}\".format(len(folds), rcll_train/len(folds)))\n",
    "    print(\"Average training AUC-ROC after {0} fold cross validation = {1}\".format(len(folds), auc_train/len(folds)))\n",
    "\n",
    "    print(\"Average validation accuracy after {0} fold cross validation = {1}\".format(len(folds), accu_valid/len(folds)))\n",
    "    print(\"Average validation precision after {0} fold cross validation = {1}\".format(len(folds), prec_valid/len(folds)))\n",
    "    print(\"Average validation recall after {0} fold cross validation = {1}\".format(len(folds), rcll_valid/len(folds)))\n",
    "    print(\"Average validation AUC-ROC after {0} fold cross validation = {1}\\n\".format(len(folds), auc_valid/len(folds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 4: Gaussian Naive Bayes with 5 fold cross validation on all versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for v in range(0, 5):\n",
    "    accu_train = prec_train = rcll_train = auc_train = 0.0\n",
    "    accu_valid = prec_valid = rcll_valid = auc_valid = 0.0\n",
    "    gnb = GaussianNB()\n",
    "    for i in range(0, len(folds)):\n",
    "        print(\"Performing fold {0}\".format(i+1))\n",
    "        gnb.fit(train_input[v][folds[i][0]], train_output[folds[i][0]])\n",
    "\n",
    "        train_pred = gnb.predict(train_input[v][folds[i][0]])\n",
    "        valid_pred = gnb.predict(train_input[v][folds[i][1]])\n",
    "\n",
    "        accu_train += accuracy_score(train_output[folds[i][0]], train_pred)\n",
    "        prec_train += precision_score(train_output[folds[i][0]], train_pred)\n",
    "        rcll_train += recall_score(train_output[folds[i][0]], train_pred)\n",
    "        auc_train += roc_auc_score(train_output[folds[i][0]], train_pred)\n",
    "    \n",
    "        accu_valid += accuracy_score(train_output[folds[i][1]], valid_pred)\n",
    "        prec_valid += precision_score(train_output[folds[i][1]], valid_pred)\n",
    "        rcll_valid += recall_score(train_output[folds[i][1]], valid_pred)\n",
    "        auc_valid += roc_auc_score(train_output[folds[i][1]], valid_pred)\n",
    "    \n",
    "    print(\"Average training accuracy after {0} fold cross validation = {1}\".format(len(folds), accu_train/len(folds)))\n",
    "    print(\"Average training precision after {0} fold cross validation = {1}\".format(len(folds), prec_train/len(folds)))\n",
    "    print(\"Average training recall after {0} fold cross validation = {1}\".format(len(folds), rcll_train/len(folds)))\n",
    "    print(\"Average training AUC-ROC after {0} fold cross validation = {1}\".format(len(folds), auc_train/len(folds)))\n",
    "\n",
    "    print(\"Average validation accuracy after {0} fold cross validation = {1}\".format(len(folds), accu_valid/len(folds)))\n",
    "    print(\"Average validation precision after {0} fold cross validation = {1}\".format(len(folds), prec_valid/len(folds)))\n",
    "    print(\"Average validation recall after {0} fold cross validation = {1}\".format(len(folds), rcll_valid/len(folds)))\n",
    "    print(\"Average validation AUC-ROC after {0} fold cross validation = {1}\\n\".format(len(folds), auc_valid/len(folds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission 1: Based on Gaussian Naive Bayes and version 4 of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of testing text data : (16476, 10)\n",
      "Size of testing text data: (16476, 10)\n",
      "Text: ['services' 'single' 'basic.4y' 'unknown' 'yes' 'yes' 'cellular' 'may'\n",
      " 'thu' 'nonexistent'] --> Data: [7 2 0 1 2 2 0 6 2 1]\n",
      "Text: ['admin.' 'single' 'high.school' 'no' 'yes' 'no' 'telephone' 'may' 'thu'\n",
      " 'nonexistent'] --> Data: [0 2 3 0 2 0 1 6 2 1]\n",
      "(16476, 19)\n",
      "Submission 1: Shape (16476, 19)\n",
      "Vector = [-0.17852255 -0.32334552  0.42203794 -0.10768784 -0.15874286  0.29193698\n",
      " -0.13943404  0.40946522  0.23854559  0.002186    0.27067439  0.06095284\n",
      " -0.1093241   0.06044387  0.2007972   0.22418289  0.27701195  0.22132551\n",
      "  0.10267718]: \n",
      "Norm = 1.0\n"
     ]
    }
   ],
   "source": [
    "test_input_raw = np.genfromtxt('testData.csv', delimiter=',', dtype=str)[1:,1:].tolist()\n",
    "for i in range(0, len(test_input_raw)):\n",
    "    for j in range(0, d-1):\n",
    "        if test_input_raw[i][j].find(\"\\\"\") != -1:\n",
    "            test_input_raw[i][j] = test_input_raw[i][j][1:-1] # removing double quotes in pairs\n",
    "        else:\n",
    "            test_input_raw[i][j] = float(test_input_raw[i][j])\n",
    "\n",
    "test_text_data = np.hstack((np.array(test_input_raw)[:,1:10], np.array(test_input_raw)[:,13:14]))\n",
    "print('Size of testing text data : {0}'.format(test_text_data.shape))\n",
    "\n",
    "# Converting the textual data into one vector per training input\n",
    "test_text_vectors = []\n",
    "\n",
    "for j in range(0, test_text_data.shape[1]):\n",
    "    lbl_enc = LabelEncoder()\n",
    "    lbl_enc.fit(train_text_data[:,j])\n",
    "    test_text_vectors.append(lbl_enc.transform(test_text_data[:,j]))\n",
    "test_text_vectors = np.array(test_text_vectors).T\n",
    "\n",
    "print('Size of testing text data: {0}'.format(test_text_vectors.shape))\n",
    "print('Text: {0} --> Data: {1}'.format(test_text_data[0], test_text_vectors[0]))\n",
    "print('Text: {0} --> Data: {1}'.format(test_text_data[10], test_text_vectors[10]))\n",
    "\n",
    "test_input_sub_raw = np.hstack((np.array(test_input_raw)[:,0:1], \n",
    "                         test_text_vectors[:,0:test_text_vectors.shape[1] - 1], \n",
    "                         np.array(test_input_raw)[:,10:13],\n",
    "                         test_text_vectors[:,test_text_vectors.shape[1] - 1].reshape(-1, 1),\n",
    "                         np.array(test_input_raw)[:,14:]\n",
    "                        )).astype(float)\n",
    "\n",
    "print(test_input_sub_raw.shape)\n",
    "test_input_sub_1 = mean_reduce.transform(test_input_sub_raw)\n",
    "test_input_sub_1 = normalize(test_input_sub_1)\n",
    "print(\"Submission 1: Shape {0}\".format(test_input_sub_1.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}'.format(test_input_sub_1[10], np.linalg.norm(test_input_sub_1[10])))\n",
    "\n",
    "sub_file = open('sub_1.csv','w')\n",
    "sub_file.write('Id,Class\\n')\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_input_4, train_output)\n",
    "sub_1_preds = gnb.predict(test_input_sub_1)\n",
    "for i in range(0, len(sub_1_preds)):\n",
    "    sub_file.write('{0},{1}\\n'.format(i+1, int(sub_1_preds[i])))\n",
    "sub_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it is time to find out feature importances:\n",
    "+ Contact will not affect a lot I guess\n",
    "+ Group data in same months (Jan, Feb, ..., Nov, Dec). Now run classifiers on each one of them and test using appropriate classifer for the test data. This is some idea taken from time series data\n",
    "+ Group ages. The behaviour in different age groups might be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jan-->[ 0.  0.]\n",
      "feb-->[ 0.  0.]\n",
      "mar-->[ 0.48  0.52]\n",
      "apr-->[ 0.78891656  0.21108344]\n",
      "may-->[ 0.93330105  0.06669895]\n",
      "jun-->[ 0.89775245  0.10224755]\n",
      "jul-->[ 0.91381292  0.08618708]\n",
      "aug-->[ 0.89336557  0.10663443]\n",
      "sep-->[ 0.55202312  0.44797688]\n",
      "oct-->[ 0.55639098  0.44360902]\n",
      "nov-->[ 0.89884868  0.10115132]\n",
      "dec-->[ 0.52727273  0.47272727]\n",
      "Cellular-->[ 13376.   2327.]\n",
      "Telephone-->[ 8552.   457.]\n"
     ]
    }
   ],
   "source": [
    "month_counter = np.zeros((12, 2))\n",
    "month_dict = {'jan': 0, 'feb': 1, 'mar': 2, 'apr': 3, 'may': 4, 'jun': 5, 'jul': 6, 'aug': 7, 'sep': 8, 'oct': 9, 'nov': 10, 'dec': 11}\n",
    "contact_counter = np.zeros((2, 2))\n",
    "for i in range(0, m):\n",
    "    month_counter[month_dict[train_input_raw[i][8]], int(train_output[i])] += 1\n",
    "    if train_input_raw[i][7] == 'cellular':\n",
    "        contact_counter[0][int(train_output[i])] += 1\n",
    "    else:\n",
    "        contact_counter[1][int(train_output[i])] += 1\n",
    "\n",
    "month_counter = month_counter.T\n",
    "month_counter = month_counter/(month_counter.sum(axis=0) + 1e-12)\n",
    "month_counter = month_counter.T\n",
    "for i in range(0, 12):\n",
    "    key = list(month_dict.keys())[list(month_dict.values()).index(i)]\n",
    "    print('{0}-->{1}'.format(key, month_counter[i]))\n",
    "    \n",
    "print('Cellular-->{0}'.format(contact_counter[0]))\n",
    "print('Telephone-->{0}'.format(contact_counter[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4702fa9a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 649 6038 7618 4519 3761 1699  166  159   85   18] --> counts\n",
      "[ 17.   24.7  32.4  40.1  47.8  55.5  63.2  70.9  78.6  86.3  94. ] --> bin edges\n"
     ]
    }
   ],
   "source": [
    "plt.hist(train_input_1[:,0], bins='auto')\n",
    "plt.show()\n",
    "hist, bin_edges = np.histogram(train_input_1[:,0])\n",
    "print('{0} --> counts'.format(hist))\n",
    "print('{0} --> bin edges'.format(bin_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few inferences:\n",
    "+ People who are contacted by cellphone subscribe 1 in approximately 8, whereas people who are contacted by telephone subscribe 1 in almost 19\n",
    "+ People in the age group 25 to 40 are contacted the most\n",
    "+ There are no people contacted in Jan and Feb and most people are contacted in May"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhU18da2LgFQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46d2cfde80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist2d, xedge, yedge = np.histogram2d(train_input_1[:,0], train_input_1[:,8])\n",
    "hist2d = hist2d.T\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "X, Y = np.meshgrid(xedge, yedge)\n",
    "ax.pcolormesh(X, Y, hist2d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe the input data is generated from a mixture of Gaussians (note the 3 spots in the 2D histogram). I am going to now train 12 classifiers. Each classifier will perform for each month of the year. Note that January and February are not in the dataset, hence we will train the classifiers for January and February based on December and March, weighing more on the data obtained in December and March respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tr_1 = pd.DataFrame(train_input_1)\n",
    "df_tr_1['Y'] = pd.Series(train_output, index=df_tr_1.index)\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "pd.tools.plotting.parallel_coordinates(df_tr_1, 'Y')\n",
    "plt.show()\n",
    "\n",
    "df_tr_2 = pd.DataFrame(train_input_2)\n",
    "df_tr_2['Y'] = pd.Series(train_output, index=df_tr_2.index)\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "pd.tools.plotting.parallel_coordinates(df_tr_2, 'Y')\n",
    "plt.show()\n",
    "\n",
    "df_tr_3 = pd.DataFrame(train_input_3)\n",
    "df_tr_3['Y'] = pd.Series(train_output, index=df_tr_3.index)\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "pd.tools.plotting.parallel_coordinates(df_tr_3, 'Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission 2: Gaussian Naive Bayes and Version 1 of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input_sub_2 = test_input_sub_raw\n",
    "sub_file = open('sub_2.csv','w')\n",
    "sub_file.write('Id,Class\\n')\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_input_1, train_output)\n",
    "sub_2_preds = gnb.predict(test_input_sub_2)\n",
    "for i in range(0, len(sub_2_preds)):\n",
    "    sub_file.write('{0},{1}\\n'.format(i+1, int(sub_2_preds[i])))\n",
    "sub_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a major issue in the pre-processing that I found out. We add the one hot encoded labels and then normalize. Larger fields could make these less \"important\", hence we have to first normalize the numeric fields first and then add the categorical one hot encoded fields. Based on this, I am defining a function to help normalize seemlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(raw_input, train_data=None, mean_reduce=False, one_norm=False, ohe=False):\n",
    "    # Extract text data present in fields\n",
    "    raw_input_text_data = np.hstack((np.array(raw_input)[:,1:10], np.array(raw_input)[:,13:14]))\n",
    "    raw_input_num_data = np.hstack((np.array(raw_input)[:,0:1], np.array(raw_input)[:,10:13], np.array(raw_input)[:,14:])).astype(float)\n",
    " \n",
    "    if train_data is not None:\n",
    "        assert len(train_data[0]) == len(raw_input[0]), \"Shape Mismatch\"\n",
    "        train_text_data = np.hstack((np.array(train_data)[:,1:10], np.array(train_data)[:,13:14]))\n",
    "        train_num_data = np.hstack((np.array(train_data)[:,0:1], np.array(train_data)[:,10:13], np.array(train_data)[:,14:])).astype(float)\n",
    "        train_text_vectors = []\n",
    "    \n",
    "    raw_input_text_vectors = []\n",
    "    for col in range(0, raw_input_text_data.shape[1]):\n",
    "        if train_data is None:\n",
    "            lbl_enc = LabelEncoder()\n",
    "            raw_input_text_vectors.append(lbl_enc.fit_transform(raw_input_text_data[:,col]))\n",
    "        else:\n",
    "            lbl_enc = LabelEncoder()\n",
    "            train_text_vectors.append(lbl_enc.fit_transform(train_text_data[:,col]))\n",
    "            raw_input_text_vectors.append(lbl_enc.transform(raw_input_text_data[:,col]))\n",
    "    raw_input_text_vectors = np.array(raw_input_text_vectors).T\n",
    "    if train_data is not None:\n",
    "        train_text_vectors = np.array(train_text_vectors).T\n",
    "            \n",
    "    # Converted text to vectors using LabelEncoder\n",
    "    # For mean reduction\n",
    "    if mean_reduce:\n",
    "        if train_data is None:\n",
    "            mean_reducer = StandardScaler()\n",
    "            raw_input_num_data = mean_reducer.fit_transform(raw_input_num_data)\n",
    "        else:\n",
    "            mean_reducer = StandardScaler()\n",
    "            mean_reducer.fit(train_num_data)\n",
    "            raw_input_num_data = mean_reducer.transform(raw_input_num_data)\n",
    "            \n",
    "    if ohe:\n",
    "        raw_input_num_data = normalize(raw_input_num_data)\n",
    "        if train_data is None:\n",
    "            one_hot_encoder = OneHotEncoder()\n",
    "            raw_input_text_ohe = one_hot_encoder.fit_transform(raw_input_text_vectors).toarray()\n",
    "        else:\n",
    "            one_hot_encoder = OneHotEncoder()\n",
    "            one_hot_encoder.fit(train_text_vectors)\n",
    "            raw_input_text_ohe = one_hot_encoder.transform(raw_input_text_vectors).toarray()\n",
    "    \n",
    "    # For normalizing\n",
    "    if one_norm:\n",
    "        if not ohe:\n",
    "            return normalize(np.hstack((raw_input_num_data, raw_input_text_vectors)).astype(float))\n",
    "        else:\n",
    "            return np.hstack((raw_input_num_data, raw_input_text_ohe)).astype(float)\n",
    "    else:\n",
    "        if not ohe:\n",
    "            return np.hstack((raw_input_num_data, raw_input_text_vectors)).astype(float)\n",
    "        else:\n",
    "            return np.hstack((raw_input_num_data, raw_input_text_ohe)).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 1: Shape (24712, 19)\n",
      "Vector = [  2.80000000e+01   1.00000000e+00   9.99000000e+02   0.00000000e+00\n",
      "  -1.70000000e+00   9.40270000e+01  -3.83000000e+01   8.90000000e-01\n",
      "   4.99160000e+03   8.00000000e+00   2.00000000e+00   6.00000000e+00\n",
      "   0.00000000e+00   2.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "   1.00000000e+00   4.00000000e+00   1.00000000e+00]: \n",
      "Norm = 5091.688836607064\n",
      "\n",
      "Version 2: Shape (24712, 19)\n",
      "Vector = [  5.49915773e-03   1.96398490e-04   1.96202092e-01   0.00000000e+00\n",
      "  -3.33877433e-04   1.84667608e-02  -7.52206217e-03   1.74794656e-04\n",
      "   9.80342704e-01   1.57118792e-03   3.92796980e-04   1.17839094e-03\n",
      "   0.00000000e+00   3.92796980e-04   0.00000000e+00   1.96398490e-04\n",
      "   1.96398490e-04   7.85593961e-04   1.96398490e-04]: \n",
      "Norm = 1.0\n",
      "\n",
      "Version 3: Shape (24712, 19)\n",
      "Vector = [-1.1518788  -0.56425354  0.19653049 -0.35249416 -1.13794396  0.7799893\n",
      "  0.48107921 -1.57424294 -2.42950204  8.          2.          6.          0.\n",
      "  2.          0.          1.          1.          4.          1.        ]: \n",
      "Norm = 11.803539349720415\n",
      "\n",
      "Version 4: Shape (24712, 19)\n",
      "Vector = [-0.09758758 -0.04780376  0.01665013 -0.02986343 -0.09640701  0.06608097\n",
      "  0.0407572  -0.13337042 -0.20582827  0.67776281  0.1694407   0.50832211\n",
      "  0.          0.1694407   0.          0.08472035  0.08472035  0.33888141\n",
      "  0.08472035]: \n",
      "Norm = 1.0\n",
      "\n",
      "Version 5: Shape (24712, 62)\n",
      "Vector = [-0.32812478 -0.16073355  0.05598377 -0.10041167 -0.32415529  0.22218815\n",
      "  0.13704047 -0.44843964 -0.69206918  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          1.          0.          0.\n",
      "  0.          0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.          1.          0.          1.\n",
      "  0.          0.          0.          0.          1.          1.          0.\n",
      "  0.          0.          1.          0.          1.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          1.          0.          1.          0.        ]: \n",
      "Norm = 3.3166247903554\n",
      "\n",
      "Fold 1\tTrain number: 19769 Validation number: 4943\n",
      "Fold 2\tTrain number: 19769 Validation number: 4943\n",
      "Fold 3\tTrain number: 19769 Validation number: 4943\n",
      "Fold 4\tTrain number: 19770 Validation number: 4942\n",
      "Fold 5\tTrain number: 19771 Validation number: 4941\n"
     ]
    }
   ],
   "source": [
    "train_input_1 = preprocess(train_input_raw)\n",
    "train_input_2 = preprocess(train_input_raw, one_norm=True)\n",
    "train_input_3 = preprocess(train_input_raw, mean_reduce=True)\n",
    "train_input_4 = preprocess(train_input_raw, mean_reduce=True, one_norm=True)\n",
    "train_input_5 = preprocess(train_input_raw, mean_reduce=True, one_norm=True, ohe=True)\n",
    "\n",
    "print(\"Version 1: Shape {0}\".format(train_input_1.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}\\n'.format(train_input_1[rand_index], np.linalg.norm(train_input_1[rand_index])))\n",
    "print(\"Version 2: Shape {0}\".format(train_input_2.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}\\n'.format(train_input_2[rand_index], np.linalg.norm(train_input_2[rand_index])))\n",
    "print(\"Version 3: Shape {0}\".format(train_input_3.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}\\n'.format(train_input_3[rand_index], np.linalg.norm(train_input_3[rand_index])))\n",
    "print(\"Version 4: Shape {0}\".format(train_input_4.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}\\n'.format(train_input_4[rand_index], np.linalg.norm(train_input_4[rand_index])))\n",
    "print(\"Version 5: Shape {0}\".format(train_input_5.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}\\n'.format(train_input_5[rand_index], np.linalg.norm(train_input_5[rand_index])))\n",
    "\n",
    "train_input = (train_input_1, train_input_2, train_input_3 , train_input_4 , train_input_5)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "folds = []\n",
    "for tr_split, va_split in skf.split(train_input_1, train_output):\n",
    "    folds.append((tr_split, va_split))\n",
    "for i in range(0, len(folds)):\n",
    "    print(\"Fold {0}\\tTrain number: {1} Validation number: {2}\".format(i+1, len(folds[i][0]), len(folds[i][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this has been implemented properly, I am going to run a few classifiers on this.\n",
    "+ Logistic Regression\n",
    "+ SVM with Gaussian Kernel (and modification of class-weights)\n",
    "+ KNN with variable number of neighbours\n",
    "+ Gaussian Naive Bayes\n",
    "+ Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 3: Logistic Regression with all versions of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in range(0, 5):\n",
    "    auc_train = auc_valid = 0.0\n",
    "    for f in folds:\n",
    "        log_reg = LogisticRegression(class_weight='balanced')\n",
    "        log_reg.fit(train_input[v][f[0]], train_output[f[0]])\n",
    "        \n",
    "        train_pred = log_reg.predict(train_input[v][f[0]])\n",
    "        valid_pred = log_reg.predict(train_input[v][f[1]])\n",
    "        \n",
    "        auc_train += roc_auc_score(train_output[f[0]], train_pred)\n",
    "        auc_valid += roc_auc_score(train_output[f[1]], valid_pred)\n",
    "    print(\"Version {0} ---> Train: {1}\\tValid: {2}\".format(v+1, auc_train/len(folds), auc_valid/len(folds)))\n",
    "    log_reg.fit(train_input[v], train_output)\n",
    "    print(\"Overall Training AUC-ROC: {0}\".format(roc_auc_score(train_output, log_reg.predict(train_input[v]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission 3: Logistic Regression and Version 4 of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_sub_3 = preprocess(raw_input=test_input_raw, train_data=train_input_raw, mean_reduce=True, one_norm=True)\n",
    "sub_file = open('sub_3.csv','w')\n",
    "sub_file.write('Id,Class\\n')\n",
    "log_reg = LogisticRegression(class_weight='balanced')\n",
    "log_reg.fit(train_input_4, train_output)\n",
    "sub_3_preds = log_reg.predict(test_input_sub_3)\n",
    "for i in range(0, len(sub_3_preds)):\n",
    "    sub_file.write('{0},{1}\\n'.format(i+1, int(sub_3_preds[i])))\n",
    "sub_file.close()\n",
    "print(\"Version 4: Shape {0}\".format(test_input_sub_3.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}\\n'.format(test_input_sub_3[29], np.linalg.norm(test_input_sub_3[29])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission 4: Logistic Regression and Version 2 of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_sub_4 = preprocess(raw_input=test_input_raw, train_data=train_input_raw, one_norm=True)\n",
    "sub_file = open('sub_4.csv', 'w')\n",
    "sub_file.write('Id,Class\\n')\n",
    "log_reg = LogisticRegression(class_weight='balanced')\n",
    "log_reg.fit(train_input_2, train_output)\n",
    "sub_4_preds = log_reg.predict(test_input_sub_4)\n",
    "for i in range(0, len(sub_4_preds)):\n",
    "    sub_file.write('{0},{1}\\n'.format(i+1, int(sub_4_preds[i])))\n",
    "sub_file.close()\n",
    "print(\"Version 4: Shape {0}\".format(test_input_sub_4.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}\\n'.format(test_input_sub_4[29], np.linalg.norm(test_input_sub_4[29])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission 5: Logistic Regression and Version 5 of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_sub_5 = preprocess(raw_input=test_input_raw, train_data=train_input_raw, mean_reduce=True, one_norm=True, ohe=True)\n",
    "sub_file = open('sub_5.csv', 'w')\n",
    "sub_file.write('Id,Class\\n')\n",
    "log_reg = LogisticRegression(class_weight='balanced')\n",
    "log_reg.fit(train_input_5, train_output)\n",
    "sub_5_preds = log_reg.predict(test_input_sub_5)\n",
    "for i in range(0, len(sub_5_preds)):\n",
    "    sub_file.write('{0},{1}\\n'.format(i+1, int(sub_5_preds[i])))\n",
    "sub_file.close()\n",
    "print(\"Version 5: Shape {0}\".format(test_input_sub_5.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}\\n'.format(test_input_sub_5[29], np.linalg.norm(test_input_sub_5[29])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 4: SVM with Gaussian and Polynomial (d = 2,3) kernels on all versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FOR GAUSSIAN KERNEL ONLY\")\n",
    "for v in range(0, 5):\n",
    "    auc_train = auc_valid = 0.0\n",
    "    for f in folds:\n",
    "        my_svm = SVC(cache_size=1000, kernel='rbf', class_weight='balanced')\n",
    "        my_svm.fit(train_input[v][f[0]], train_output[f[0]])\n",
    "        \n",
    "        train_pred = my_svm.predict(train_input[v][f[0]])\n",
    "        valid_pred = my_svm.predict(train_input[v][f[1]])\n",
    "        \n",
    "        auc_train += roc_auc_score(train_output[f[0]], train_pred)\n",
    "        auc_valid += roc_auc_score(train_output[f[1]], valid_pred)\n",
    "    print(\"Version {0} ---> Train: {1}\\tValid: {2}\".format(v+1, auc_train/len(folds), auc_valid/len(folds)))\n",
    "    my_svm.fit(train_input[v], train_output)\n",
    "    print(\"Overall Training AUC-ROC: {0}\".format(roc_auc_score(train_output, my_svm.predict(train_input[v]))))\n",
    "    \n",
    "print(\"FOR POLYNOMIAL KERNELS ONLY\")\n",
    "for degree in [2, 3, 5]:\n",
    "    print(\"Polynomial Kernel: Degree {0}\".format(degree))\n",
    "    for v in range(0, 5):\n",
    "        auc_train = auc_valid = 0.0\n",
    "        for f in folds:\n",
    "            my_svm = SVC(cache_size=1000, kernel='poly', degree=degree, class_weight='balanced')\n",
    "            my_svm.fit(train_input[v][f[0]], train_output[f[0]])\n",
    "        \n",
    "            train_pred = my_svm.predict(train_input[v][f[0]])\n",
    "            valid_pred = my_svm.predict(train_input[v][f[1]])\n",
    "        \n",
    "            auc_train += roc_auc_score(train_output[f[0]], train_pred)\n",
    "            auc_valid += roc_auc_score(train_output[f[1]], valid_pred)\n",
    "        print(\"Version {0} ---> Train: {1}\\tValid: {2}\".format(v+1, auc_train/len(folds), auc_valid/len(folds)))\n",
    "        my_svm.fit(train_input[v], train_output)\n",
    "        print(\"Overall Training AUC-ROC: {0}\".format(roc_auc_score(train_output, my_svm.predict(train_input[v]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission 6: SVM with Polynomial Kernel of Degree 2 with version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742758235416\n",
      "Version 5: Shape (16476, 19)\n",
      "Vector = [  6.56828403e-04  -5.64253535e-01   1.96530488e-01  -3.52494164e-01\n",
      "  -1.20170736e+00  -8.68859300e-01  -1.42755539e+00  -1.27745458e+00\n",
      "  -9.41233821e-01   9.00000000e+00   1.00000000e+00   5.00000000e+00\n",
      "   0.00000000e+00   2.00000000e+00   2.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.00000000e+00   1.00000000e+00]: \n",
      "Norm = 11.14612047418492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_input_sub_6 = preprocess(raw_input=test_input_raw, train_data=train_input_raw, mean_reduce=True)\n",
    "sub_file = open('sub_6.csv', 'w')\n",
    "sub_file.write('Id,Class\\n')\n",
    "p_svm = SVC(kernel='poly', cache_size=1000, degree=2, class_weight='balanced')\n",
    "p_svm.fit(train_input_3, train_output)\n",
    "print(roc_auc_score(train_output, p_svm.predict(train_input_3)))\n",
    "sub_6_preds = p_svm.predict(test_input_sub_6)\n",
    "for i in range(0, len(sub_6_preds)):\n",
    "    sub_file.write('{0},{1}\\n'.format(i+1, int(sub_6_preds[i])))\n",
    "sub_file.close()\n",
    "print(\"Version 5: Shape {0}\".format(test_input_sub_6.shape))\n",
    "print('Vector = {0}: \\nNorm = {1}\\n'.format(test_input_sub_6[29], np.linalg.norm(test_input_sub_6[29])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
