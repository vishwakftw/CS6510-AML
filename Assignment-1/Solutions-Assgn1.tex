\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Solutions to Assignment 1 : CS6510 - Applied Machine Learning}
\author{Vishwak S\\
\texttt{CS15BTECH11043}}
\date{}

\begin{document}
\maketitle

\section*{Question 1}
\subsection*{Part 1}
\begin{flushleft}
Since \(X\) and \(Y\) are independent, \(P(X)\times P(Y) = P(X \cap Y)\). \(P(\bar{X}) = 1 - P(X)\). Due to this:
\begin{gather*}
P(\bar{X} \cap Y) = P(Y) - P(X \cap Y) \quad \text{ (from Set Theory)} \\
\implies P(\bar{X} \cap Y) = P(Y) - P(Y)\times P(X) = P(Y)(1 - P(X)) = P(Y) \times P(\bar{X}) 
\end{gather*}

Hence \(\bar{X}\) and \(Y\) are independent.
\end{flushleft}

\subsection*{Part 2}
\begin{flushleft}
Listing down the information from the question:
\begin{center}
\begin{tabular}{c c c c}
\(P(C1 = H) \) & \(= 0.5\) & \(P(C1 = T) \) & \( = 0.5\) \\
\(P(C2 = H | C1 = H) \) & \(= 0.7\) & \(P(C2 = H | C1 = T) \) & \( = 0.5\) 
\end{tabular}
\end{center}

From Bayes' Theorem: \(P(A | B) = \displaystyle \frac{P(B | A) P(A)}{P(B)}\). We need to find: \(P(C1 = T \cap C2 = H)\).
\begin{gather*}
P(C1 = T \cap C2 = H) = P(C2 = H) \times P(C1 = T | C2 = H) \quad \text{ (from Conditional probability)} \\
\implies P(C1 = T \cap C2 = H) = P(C2 = H) \times \displaystyle \frac{P(C2 = H | C1 = T) P(C1 = T)}{P(C2 = H)} \quad \text{ (from Bayes' Theorem)}\\
\implies P(C1 = T \cap C2 = H) = P(C1 = T | C2 = H) P(C1 = T) = 0.5 \times 0.5 = \boxed{0.25}
\end{gather*}
\end{flushleft}

\section*{Question 2}
\subsection*{Part a}
\begin{flushleft}
A set \(S\) is a vector space if the following conditions hold:
\begin{itemize}
\item if \(u, v \in S\), then \(u + v \in S\).
\item if \(u \in S\), then \(\alpha u \in S \quad \forall \alpha\).
\end{itemize}

\(\nexists u, v \in S = \emptyset\), hence the first condition holds [\texttt{False} \(\implies\) \texttt{True} is \texttt{True}]. Similarly, \(\nexists u \in S = \emptyset\), hence the second condition holds for the same reason above. Hence the empty set is a vector space.
\end{flushleft}

\subsection*{Part b and c}
\begin{flushleft}
Let \(M^{-1}\) be of the form: \(I + \alpha(\mathbf{u} \mathbf{v}^{T})\). We know that: \(M M^{-1} = I\). Applying this we get:
\begin{gather*}
M  M^{-1} = I \\
(I + \mathbf{u}\mathbf{v}^T) (I + \alpha(\mathbf{u}\mathbf{v}^{T})) = I \\
\implies I + \mathbf{u}\mathbf{v}^{T}(1 + \alpha) + \alpha\mathbf{u}(\mathbf{v}^{T} \mathbf{u})\mathbf{v}^{T} = I \\
\implies \mathbf{u}\mathbf{v}^{T}\left((1 + \alpha) + \alpha(\mathbf{v}^{T}\mathbf{u})\right) = \mathbf{0} \\
\implies \alpha = \displaystyle \frac{-1}{1 + \mathbf{v}^{T} \mathbf{u}} 
\end{gather*}

Since there is an \(\alpha\), we can tell that our assumption is true, and hence the inverse of the matrix \(M\) is of the form \(I + \alpha(\mathbf{u}\mathbf{v}^{T})\). 
\end{flushleft}

\subsection*{Part d and e}
\begin{flushleft}
If \(M\) is singular: then \(\text{det}M = 0\). We know that the eigenvalues of \(M = I + \mathbf{u}\mathbf{v}^{T}\) are of the form: \(1 + \lambda_{i}\), where \(\lambda_{i}\)s are the eigenvalues of \(\mathbf{u}\mathbf{v}^{T}\). We also know that the determinant of the matrix is the product of its eigenvalues.
\begin{gather*}
\text{det}M = 0 \implies \displaystyle \prod_{i=1}^{n} (1 + \lambda_{i}) = 0 \\
\implies \exists \lambda_{k} \text{ such that }(1 + \lambda_{k}) = 0
\end{gather*}

This means that at least one of the eigenvalues of \(\mathbf{u}\mathbf{v}^{T}\) is exactly \(-1\). From the expression for \(\alpha\), we know that this definitely happens when \(\mathbf{v}^{T} \mathbf{u} = -1\), in which case, \(\alpha\) is undefined. 
\(\newline\)

The Null space of \(M\) is defined as: \(M \mathbf{x} = \mathbf{0}\). This means that:
\begin{gather*}
(I + \mathbf{u}\mathbf{v}^{T}) \mathbf{x} = \mathbf{0} \\
\mathbf{x} + \mathbf{u}\mathbf{v}^{T} \mathbf{x} = \mathbf{0} 
\end{gather*}

Recall that if \(M\) is singular, then \(-1\) is an eigenvalue of \(\mathbf{u} \mathbf{v}^{T}\) and hence: \(\mathbf{u}\mathbf{v}^{T}\mathbf{x} = -\mathbf{x}\). 

Resuming from where we left off: consider \(\mathbf{x} = \mathbf{u}\). \(\mathbf{u} - \mathbf{u} = 0 \text{ (since }\mathbf{v}^{T}\mathbf{u} = -1 \text{)} \).
\end{flushleft}

\section*{Question 3}
\subsection*{Part a}
\begin{flushleft}
\begin{equation*}
A = \begin{bmatrix} 
-2 & 2 \\
-6 & 5
\end{bmatrix}
\end{equation*}

The eigenvalues of \(A\) are given by the roots to the equation \(\text{det}(A - \lambda I) = 0\). We get: \(\lambda^2 - 3\lambda + 2 = 0 \implies \lambda = 1, 2\). The eigenvectors are solutions to the equation:
\begin{equation*}
A \mathbf{x} = \lambda_{i} \mathbf{x} 
\end{equation*}

For \(\lambda = 1\):
\begin{gather*}
A \mathbf{x} = \mathbf{x} \implies \begin{bmatrix} -2 & 2 \\ -6 & 5 \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = \begin{bmatrix} a \\ b \end{bmatrix} \\
\mathbf{x} = a \begin{bmatrix} 1 \\ \frac{3}{2} \end{bmatrix} \quad \forall a
\end{gather*}

For \(\lambda = 2\):
\begin{gather*}
A \mathbf{x} = 2\mathbf{x} \implies \begin{bmatrix} -2 & 2 \\ -6 & 5 \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = \begin{bmatrix} 2a \\ 2b \end{bmatrix} \\
\mathbf{x} = a \begin{bmatrix} 1 \\ 2 \end{bmatrix} \quad \forall a
\end{gather*}
\end{flushleft}

\subsection*{Part b}
\begin{flushleft}
Let \(U = \begin{bmatrix} u_{1} & u_{2} \\ u_{3} & u_{4} \end{bmatrix}\).
Solving the system:
\begin{gather*}
AU = U\Lambda \\
\implies u_{3} = 3\frac{u_{1}}{2} \text{ and } u_{4} = 2u_{2} \\
\end{gather*}

\(U\) is the matrix of the eigenvectors of \(A\). For simplicity, consider: \(u_{1} = 2\gamma, u_{2} = \zeta \implies u_{3} = 3\gamma, u_{4} = 2\zeta\). Hence:
\begin{equation*}
U = \begin{bmatrix}
2\gamma & \zeta \\ 3\gamma & 2\zeta 
\end{bmatrix}
\end{equation*}
\end{flushleft}

\subsection*{Part c}
\begin{flushleft}
For an invertible \(2 \times 2\) matrix \(A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}\), the inverse is:
\begin{equation*}
A^{-1} = \frac{1}{ad - bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} 
\end{equation*}

This can be found using the transpose of the co-factor matrix (adjoint). In our case:
\begin{gather*}
U^{-1} = \frac{1}{\gamma \zeta} \begin{bmatrix} \zeta & -\zeta \\ -3\gamma & 2\gamma \end{bmatrix} \\
U\Lambda U^{-1} = \begin{bmatrix} 2\gamma & 2\zeta \\ 3\gamma & 4\zeta \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} 2\zeta & -\zeta \\ -3\gamma & 2\gamma \end{bmatrix} \frac{1}{\gamma \zeta}\\
\implies U\Lambda U^{-1} = \begin{bmatrix} 2\gamma & 2\zeta \\ 3\gamma & 4 \zeta \end{bmatrix} \begin{bmatrix} 2\zeta & -\zeta \\ -3\gamma & 2\gamma \end{bmatrix} \frac{1}{\gamma \zeta} = \frac{1}{\gamma \zeta} \begin{bmatrix} -2\gamma \zeta & 2\gamma \zeta \\ -6\gamma \zeta & 5\gamma\zeta\end{bmatrix} \\
\implies U\Lambda U^{-1} = \begin{bmatrix} -2 & 2 \\ -6 & 5 \end{bmatrix} = A
\end{gather*}

Verified. 
\end{flushleft}
\end{document}
