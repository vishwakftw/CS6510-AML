\documentclass{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}

\title{Solutions to Assignment 2 : CS6510 - Applied Machine Learning}
\author{Vishwak S\\
\texttt{CS15BTECH11043}}
\date{}

\begin{document}
\maketitle

\section*{Question 2}
\subsection*{Part a}
The dendrogram is below, circles represent clusters.
\begin{figure}[H]
\centering
\includegraphics[width=0.3\linewidth]{./images/2a.png}
\end{figure}

\subsection*{Part b}
The dendrogram is below, circles represent clusters.
\begin{figure}[H]
\centering
\includegraphics[width=0.3\linewidth]{./images/2b.png}
\end{figure}

\subsection*{Part c}
Changing the lowest distance i.e., \(d(x1, x2) = 0.12 \rightarrow 0.09\) and the highest distance i.e., \(d(x3, x6) = 0.93 \rightarrow 0.96\) will not affect the answers.

\section*{Question 3}
\subsection*{Part a}
\begin{flushleft}
Below is the neural network that achieves the desired output i.e., the \texttt{XOR} function. 
\begin{figure}[H]
\centering
\includegraphics[width=0.4\linewidth]{./images/3a.png}
\end{figure}
Activations are present in both the hidden and output layers.
\end{flushleft}

\subsection*{Part b}
\begin{flushleft}
Below is the neural network that achieves the desired output i.e., the \texttt{XOR} function.
\begin{figure}[H]
\centering
\includegraphics[width=0.4\linewidth]{./images/3b.png}
\end{figure}
Activations are present in both the hidden and output layers.

For both Part a and Part b, there is a supporting script that will help you verify if the answers are correct or not. This is the file \texttt{checker.py}.
\end{flushleft}

\subsection*{Part c}
\subsubsection*{Part i}
Note that \(\sigma(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} = \frac{1 - e^{-2z}}{1 + e^{-2z}} = 2\times\text{sigmoid}(2x) - 1\). Using this fact we get:
\begin{gather}
\sigma'(z) = 2\times \text{sigmoid}'(x) = 2\times 2\times \text{sigmoid}(2x)\times(1 - \text{sigmoid}(2x)) = 2\times(\tanh(z) + 1)\left(1 - \frac{\tanh(z) + 1}{2}\right) \\
\sigma'(z) = (1 - \tanh(z))(1 + \tanh(z)) = \boxed{(1 - \sigma(z))(1 + \sigma(z))}
\end{gather}
\subsubsection*{Part ii}
\begin{flushleft}
Note that node after the softmax applied for a layer is as follows, considering \(K\) nodes:
\begin{gather}
p_{i} = \frac{e^{z_{i}}}{e^{z_{i}} + N} \\
\frac{\partial p_{i}}{\partial z_{j}} = \begin{cases} \frac{Ne^{z_{i}}}{(e^{z_{i}} + N)^2} = (1 - p_{i})(p_{i}) & \text{ if i = j } \\ -\frac{e^{z_{i}}e^{z_{j}}}{(e^{z_{i}} + N)^2} = -p_{i}p_{j} & \text{ otherwise } \end{cases}
\end{gather}
where N is the rest of the normalization factor (written for convenience).

Using these two equations:
\begin{equation}
\frac{\partial L}{\partial z_{j}} = -\displaystyle \sum_{i=1}^{K} \frac{y_{i}}{p_{i}} \frac{\partial p_{i}}{\partial z_{j}} = -y_{i}(1 - p_{i}) + \sum_{i=1 , i \neq j}^{K} p_{j}y_{i} = -y_{i} + \sum_{i=1}^{K} p_{j}y_{i} = -y_{i} + p_{j}\sum_{i=1}^{K}y_{i} = \boxed{p_{j} - y_{i}}
\end{equation}

In the third equality, \(y_{i}p_{i} = y_{i}p_{j}\) if \(j = i\). This term is pushed inside the summation, for the rest of the equation.
\end{flushleft}
\end{document}
